# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021 DaoAI Robotics Inc.
# This file is distributed under the same license as the DaoAI Vision User
# Manual package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DaoAI Vision User Manual \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-05-26 15:26-0700\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../template-usage/2d-mod-finder/overview.rst:2
msgid "Gray Mod Finder"
msgstr ""

#: ../template-usage/2d-mod-finder/overview.rst:4
msgid ""
"It used the gray image to find the object in 2d, then project into 3d "
"space."
msgstr ""

#: ../template-usage/3d-calibration/overview.rst:2
msgid "3D calibration overview"
msgstr ""

#: ../template-usage/3d-mod-finder/overview.rst:2
msgid "Depth Mod Finder"
msgstr ""

#: ../template-usage/3d-mod-finder/overview.rst:4
msgid ""
"It used the depth information to find the object and locate the object in"
" 3d space."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:2
msgid "Deep Learning KungFuTea Tea Bag Picking"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:4
msgid ""
"This article is a sample project for Deep Learning engine detection & "
"picking. If you want to know more detail about Deep Learning, you can "
"checkout `this article <https://daoai-robotics-inc-daoai-vision-user-"
"manual.readthedocs-hosted.com/en/latest/deep-learning/index.html>`_ "
"before continue reading; We will use the **Reconstruct** node to process "
"the 3D coordinate. If you want to know different picking engines using "
"DL, checkout `these articles <https://daoai-robotics-inc-daoai-vision-"
"user-manual.readthedocs-hosted.com/en/latest/complete-vision-"
"guidance/detection/overview.html#deep-learning-engines>`_."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:9
msgid "Project Description"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:14
msgid "The requirement of this sample project is that:"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:16
msgid "Capture one image robot perform picking for one object;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:17
msgid ""
"You select the designated type of object, robot picks and only picks this"
" type of object;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:18
msgid "Picking from the highest;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:19
msgid "Collision free;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:21
msgid ""
"You can open and modify the **RGB Mod Finder template** (or any other "
"template which has **pose_generation** & **picking** flowcharts). In this"
" article, we will use RGB Mod Finder as our example. Some of the "
"structures from template can be used to fit our project. In our example, "
"the picking, pose generation and pose define flowcharts are giving the "
"same functionalities to our project. Therefore, you only need to change "
"detection flowchart(**Not** all the scenarios are the same, sometimes you"
" still need to change other flowcharts in order to meet the "
"requirements)."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:27
msgid ""
"Starting from version 2.22.4.0 **Vision Studio**, **Interface** node "
"supports the generic input/output handling which can simplify the "
"dependencies between flowcharts. This will be discuss later in this "
"article."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:30
msgid "Collect Data"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:32
msgid ""
"In order to check your workspace is working properly as you wish, you "
"should prepare large enough amount of data to test your workspace. "
"Depending on the complexity of your project, you might need to collect "
"more than hundreds of testing data for your workspace."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:35
msgid ""
"You should try to simulate the scene as close as to project 's actual "
"working environment, and try to cover all the possible occurrences from "
"actual working environment. Save your testing images into local "
"directory, and they will be really useful soon!"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:38
msgid ""
"If you want to know how to save image data? Checkout `this section "
"<TODO>`_ before you continuous reading."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:41
msgid "Changing flowcharts"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:43
msgid "Next step, you can start working on the specific project requirements."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:45
msgid ""
"You can delete your detection flowchart on the template, then adding a "
"new flowchart renamed as **detection**."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:48
msgid ""
"Try to use the same flowchart name when you deleted and added a new "
"flowchart. Naming them the same will save you lots of work, since many "
"different nodes is linking to detection flowchart. If you change the "
"name, even just capitalize one character, the link expression is no "
"longer valid. You will need to change all the links in your workspace. "
"**So, think carefully when you name the flowchart!**"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:50
msgid ""
"After that, you can start adding nodes into the brand new flowchart. "
"**BUT!!!** Beforehand, you should plan and consider: what kind of "
"detection engine you need? And why? Can you optimize the productivity of "
"this detection flowchart?"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:52
msgid ""
"When considering these questions, you should come back into the "
"requirements."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:59
msgid "**What kind of objects you need to pick?**"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:55
msgid ""
"Object is tea bags and snack bags which either has soft or solid surface."
" Since this project has different kinds of objects and surfaces. A "
"suction gripper to suck up the object from its surface. Furthermore, "
"since these objects are different in appearance, it is difficult to use "
"**Mod Finder**. Unless, you define 20 models in **Mod Finder**. But if "
"you do this, it is nearly impossible to manage & low in productivity. "
"Therefore Deep Learning is a better method to detect these objects."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:64
msgid ""
"**How many objects your robot will be picking with one image captured? "
"What is the order of picking all these objects?**"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:62
msgid ""
"From requirements, robot will pick **ONE** occurrence of the object. "
"Therefore, theoretically you will only need to detect one object in "
"scene. However, if you only detect one object occurrence, this object "
"maybe laying underneath another object. This is not a safe case for robot"
" to perform picking. Hence, you still need to detect all the occurrences "
"of objects then sort these poses from highest Z value to pick."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:66
msgid "**Is it possible to occur collisions during picking?**"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:73
msgid ""
"From the image above we can see that objects will be laying in the "
"box/basket. Vision is unable to detect the box like edges/walls. You will"
" need to define the box/basket edges/walls for Vision. Hence, collision "
"handling is necessary."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:78
#: ../template-usage/mono-3d/mono-3d-template.rst:119
msgid "Teach Pose"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:83
msgid ""
"This is the pose define flowchart pipeline. The purpose of this flowchart"
" is that: you can define the picking pose for objects. You should define "
"all the poses if different objects have different picking poses. In our "
"example project, suction gripper can work generally on all these objects."
" So we only need one pose for this project."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:89
msgid "This flowchart outputs the relation of ``Gripper`` to ``Object``."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:97
msgid "General Description of the pipeline:"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:92
msgid "Loading calibration file for current camera and robot base relation;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:93
msgid ""
"Loading the gripper mesh and object mesh(this is optional for better "
"visualization, but mesh/cloud inputs are required for **Gripper** node);"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:94
msgid ""
"Executing the detection flowchart to calculate the Object in Cloud/Camera"
" relation;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:95
msgid ""
"**(Optional)** Moving robot to the pose just like showing robot \"how to "
"pick\" the object;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:96
msgid ""
"**(Optional)** Transforming these relations with/without robot picking "
"pose(You can feed the robot picking pose);"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:97
msgid ""
"Feeding these results to **Gripper** to generate our final Gripper to "
"Object relation;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:99
msgid "Detail steps are like below:"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:104
msgid ""
"The ``.yml`` file name which contains the current Camera to Robot Base "
"relation;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:109
msgid ""
"Loading the gripper mesh(``.stl`` or ``.ply`` file), Same for object "
"mesh/cloud model(``pcd`` or ``ply`` file). In this example, we used a "
"plane as our object mesh for this visualization;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:114
msgid "Executing the detection flowchart to get Object to Cloud relation;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:119
msgid "**(Optional)** Robot picking pose;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:121
msgid ""
"After the transformation process, here comes the interesting part. And "
"this is the main operations you will need to take care of: the "
"**Gripper** node."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:126
msgid ""
"First of all, the **Gripper** node requires gripper and object models. It"
" will display the current setting's visualization of Gripper to Object "
"relation. If you used the robot picking pose above, you should click the "
"**Use initial Pose** and you will see the visualization of the pose you "
"taught robot."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:132
msgid ""
"You can adjust the current pose from **Adjust Pose on Display**. You can "
"also enter the corresponding x, y, z, rx, ry, rz values for this pose. "
"TCP in flange relation is also available to tune in here, which can "
"increase the accuracy for some situations."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:138
msgid ""
"Here is an example pick pose as Gripper to Object relation(I used the "
"ball mesh for object since it is better for demonstration)."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:141
msgid "Deep Learning"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:146
msgid ""
"As discussed above, we should use Deep Learning Engine for our detection."
" You can checkout these article to see how to `collect data <https"
"://daoai-robotics-inc-daoai-vision-user-manual.readthedocs-"
"hosted.com/en/latest/deep-learning/dataset.html#>`_, `annotate your "
"dataset <https://daoai-robotics-inc-daoai-vision-user-manual.readthedocs-"
"hosted.com/en/latest/deep-learning/annotation/index.html>`_ and `train "
"your model <https://daoai-robotics-inc-daoai-vision-user-manual"
".readthedocs-hosted.com/en/latest/deep-learning/jenkins-usage.html>`_."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:152
#: ../template-usage/mono-3d/mono-3d-template.rst:133
msgid "Picking"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:155
msgid ""
"The flowchart **picking** has became **main_flowchart** and flowchart "
"**pose generation** has became **picking** in version newer than "
"**2.22.4.0**."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:160
msgid ""
"Generally, the picking process is pretty similar in terms of workflow. "
"Define a gripper pose at the beginning, then image captures, then process"
" through Vision algorithm; performs picking."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:170
msgid ""
"For Pose Define flowchart, we need to load the calibration file for "
"current camera and robot base relation. As well as calculate the gripper "
"pose:"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:167
msgid ""
"**Calibration**, **Sphere Calibration** and **DA Calibration** loads the "
"``.yml`` file and archive the relation from the file;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:168
msgid "**Reader** nodes loading the Gripper mesh and Object mesh/cloud files;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:169
msgid ""
"**(Optional) Detection flowchart** and **Robot Read** to teach Vision "
"about the picking pose;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:170
msgid "**Gripper** node calculates the picking pose;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:172
msgid ""
"After pose define flowchart, you will have **Camera in Base** and **Tool "
"in Object** relations."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:181
msgid ""
"For detection flowchart, we have simplified and keep the core "
"functionalities in it:"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:178
msgid "**Camera** node -> Image Captures;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:179
msgid "**DL Segment** node -> Finding objects by the deep learning models;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:180
msgid ""
"(Optional) **Cloud Process** node -> Cropping object models or define "
"custom reference frame;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:181
msgid "**Reconstruct** node -> Generating 3D coordinates;"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:183
msgid "After detection flowchart, you will have **Object in Cloud** relation."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:186
msgid ""
"Camera and Cloud is a 180 rotation of x-axis. Therefore, once we have "
"**Object in Cloud** relation, we also have **Object in Camera** relation."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:191
msgid ""
"For **pose_generation** flowchart, it is basically the same pipeline. But"
" the link expressions will need to change followed with detection "
"flowchart. Collision Avoidance and Transformation Tree node will need "
"input from Reconstruct node."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:196
msgid ""
"If you are using 2.22.4.0 or newer version, you will not need to worry "
"about link expressions since you have Interface node to take care of the "
"inputs/outputs."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:198
msgid ""
"Now you have **Camera in Base**, **Tool in Object** and **Object in "
"Cloud/Camera** relations, last thing to do is transforming all these "
"relations and calculate the final goal: **Tool in Base** relation and "
"send it to the robot. **Transformation Tree** node is able to do the "
"calculation. Therefore, you can apply this output to **Robot Write**."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:205
msgid ""
"This is the modified picking flowchart for this project, since we only "
"need these functionalities to perform picking. You can also use the "
"picking flowchart from template, it only has minor changes in order to "
"get it working."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:209
msgid "Interface node"
msgstr ""

#: ../template-usage/KungFuTea/index.rst:214
msgid ""
"If you are using Vision Studio version newer than 2.22.4.0, you should be"
" able to add **Interface** node to manage the outputs across different "
"flowcharts."
msgstr ""

#: ../template-usage/KungFuTea/index.rst:216
msgid ""
"In this example project, the essential changes is in detection flowchart."
" When one flowchart is using another flowchart's output, you can link it "
"through **Interface** node. Therefore, changing detection flowchart does "
"not affect the link expressions on other flowcharts. It would be a good "
"practice to decrease coupling between flowcharts."
msgstr ""

#: ../template-usage/index.rst:2
msgid "Template usage"
msgstr ""

#: ../template-usage/index.rst:3
msgid ""
"This section we will introduce the usage and example of templates. "
"According to the sample requirements and projects, we will demonstrate "
"how to apply and modify the templates in order to fulfill requirements."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:2
msgid "Mono 3d template usage"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:4
msgid "It used the 2d feature to extract the 3d position of the object."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:6
msgid ""
"(reference https://docs.pickit3d.com/en/latest/examples/build-a-showcase-"
"demo-with-a-m-camera-and-suction-cup.html)"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:9
msgid "Use Case"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:10
msgid "TODO: Upload Benz Battery project video after done."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:13
msgid "Requirement"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:14
msgid "TODO: Abstract Benz Battery project Requirements after done."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:17
msgid "Mounting instructions"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:18
msgid "TODO: Abstract Benz Battery project instructions after done."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:21
msgid "Setting up the picking pipeline"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:23
msgid "Calibrate the camera"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:24
msgid ""
"Nest step is the Hand-Eye Calibration. This process must use the DA "
"Calibration nodes. It generates the relative positions between the robot "
"flange and the camera. This relation is used to transform the object pick"
" points into robot coordinates. Details can be found in .. _my-reference-"
"label: TODO: Add DA Calibration reference rst."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:27
msgid "Define the feature and train the object"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:28
msgid ""
"After Hand-Eye Calibration, head to :ref:`mono-3d<Placing the object "
"under the camera>` and follow the instructions to define features and "
"train/set an object model."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:31
msgid "Teach the picking pose"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:56
msgid ""
"Teach picking pose process is made to find the relative positions between"
" the robot flange and the object. Once this relation is generated, the "
"relative position between tool and object will remain the same while "
"picking."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:33
msgid ""
"Head to the **Teach_Pose** flowchart and click **DA Calibration** node. "
"On the configuration window, select Hand Eye Config setting to be eye-in-"
"hand, and enter the DA calibration output file name."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:34
msgid ""
"Click **Manage Variables** button and set Mode to **2**, and 2_Step to "
"**False** by unchecking the value box."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:39
msgid ""
"Click on the first reader node and load browse the gripper mesh file. "
"Then click on the second reader node and load the object mesh file."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:44
msgid ""
"If there's no gripper and object mesh file matching the real ones, load "
"any mesh file."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:46
msgid ""
"Click on the **Mono 3D** node pose estimate mode, set the calibration "
"Context and mono training file name just like what was defined in "
"**Detection** flowchart."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:51
msgid ""
"Use **Next Step** to run the flowchart step by step, first **Robot Read**"
" node require the robot to send the detection pose which is the pose "
"where the camera captures. The second **Robot Read** node require the "
"robot to send the picking pose of how gripper should pick the object."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:53
msgid ""
"After running the first **Gripper** node, click on it, and click on the "
"existing pose, click the trash icon button to remove previously saved "
"poses. Then click ** + TCP in Object Pose**, and click the **View** "
"button to see preview of relative position between tool and object."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:58
msgid "Ignore the second gripper. It is used for 2 step picking."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:61
msgid "Execute the picking"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:63
msgid ""
"Click **Manage Variables** button and set Mode to **3**, and 2_Step to "
"**False** by unchecking the value box."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:69
msgid ""
"Get Robot picking script ready and run the **Picking** flowchart. UR "
"Picking Experiment Script example below:"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:75
msgid "Flowchart Summary"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:98
msgid "Manual"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:99
msgid ""
"The data flow for this flowchart is basically gathering the camera "
"captured image, mod finder result, and robot pose into **Mono 3D** "
"Accumulate mode and use **Mono 3D** final mode to generate a training "
"file."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:105
msgid "Mono Train"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:106
msgid ""
"The data flow for this flowchart is similar to **Manual**. Instead of "
"acquiring image from camera and pose from robot read, it gets data from "
"assembled bag, plus mod finder result into **Mono 3D** Accumulate mode "
"and use **Mono 3D** final mode to generate a training file. If **Mono "
"3D** Set feature mode is used, none of the data will be needed."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:112
msgid "Mod Finder"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:113
msgid ""
"The image input for **Mod Finder** nodes comes from different flowchart "
"depending on the value of Variable.Mode. Then Second **Mod Finder** node "
"uses first one as it's reference fixture which anchors the geo features. "
"Then the output goes back to different flowcharts."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:120
msgid ""
"Firstly, **DA Calibration** node will load the relative position between "
"camera and flange. Secondly, **Camera** node will provide image for **Mod"
" Finder** flowchart which will generate geo features in camera 2D "
"location for **Mono 3D** pose estimate mode. First **Robot Read** node "
"will read the robot pose for detection pose, second **Robot Read** node "
"will read the robot pose for picking pose, and both will be passed to "
"**Transformation Tree* node. **Mono 3D** pose estimate mode will generate"
" the object in camera 3D location for **Transformation Tree** node. "
"Afterwards, **Transformation Tree** node will calculate the Flange in "
"object relative position and pass it to **Gripper** node. Since "
"**Gripper** node is added to recipe, the saved pose will be loaded "
"through **Load Recipe** node in **Detection** flowchart."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:126
msgid "Detection"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:127
msgid ""
"The **Load Recipe** node will load **Calibration** and **Gripper** node "
"output from **Teach Pose** flowchart. Then Mono 3D will gather detection "
"pose, image, and mod finder result to generate the actual object in "
"camera location."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:134
msgid ""
"The **Robot Read** node will receive the detection pose and pass it to "
"**Transformation Tree** node. **Mono 3D** node in **Detection** flowchart"
" will provide object in camera 3D location for **Transformation Tree** "
"node. The **Gripper** and **DA Calibration** node will be loaded through "
"**Load Recipe** node and provide camera in tool and tool in object "
"location. Finally **Transformation Tree** node will generate the tool in "
"base and guide robot to pick the object."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:140
msgid "Cautions"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:142
msgid "2 Step Picking"
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:143
msgid ""
"This document is only for 1 step picking. Two step picking is only for "
"high accuracy requirement which the first step is to move camera to a "
"better detection position. In the switch node of **Picking** flowchart, "
"case_1 is to generate the better detection pose, and payload from robot "
"will be needed to switch first and second step. The 2 Step Picking will "
"be enabled by changing the **Variable.Mode** to True."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:148
msgid "The **Mod Finder** nodes input image is decided by the **Variable.Mode**."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:153
msgid ""
"The **Switch** node in **Mono Train** flowchart will be evaluated to True"
" if the **Count** node in front of it is equals the number of bags set in"
" **Constant** node."
msgstr ""

#: ../template-usage/mono-3d/mono-3d-template.rst:158
msgid ""
"The **Manual** flowchart will be using the same robot script as Manual "
"Calibrations. The switch node will check the received command from "
"**Robot Read** node."
msgstr ""

