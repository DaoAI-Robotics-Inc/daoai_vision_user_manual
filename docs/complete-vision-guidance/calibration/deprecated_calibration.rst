Calibration
===========

Calibration is the process by which the camera and the robot determine their relative positions in relation to one another, allowing the camera to direct the robot to the proper positions within the work cell.
Calibration can begin once the camera and robot are properly installed in the work cell. Calibration can be performed Eye-to-hand (flanged on the frame) or Eye-in-hand (mounted on the camera) in order to produce a coordinate system between the camera and the robot base, or between the camera and the gripper, depending on the case scenario. 

.. Attention:: 
    The coordinating system for both case scenarios need to be reset whenever the robot base gets moved or the camera gets taken off from the gripper. 

Sphere Calibration 
-------------------

Overview
~~~~~~~~~

The sphere calibration procedure establishes a coordinate system between the camera and the robot base (Eye to hand), or between the camera and the gripper (Eye in hand). Given that we will be detecting things in the images and that the images can only be translated directly to the camera coordinates, we will need a method for informing the robot about the location of the items in the actual world. And this can be accomplished by evaluating the object's accuracy in the scene cloud generated by the camera using the sphere mesh and its point cloud. Depending on the application type, the object's position will be displayed in relative coordinates.

Preparation
~~~~~~~~~~~~

* To obtain the best results, it is necessary to prepare the laboratory environment prior to running the sphere calibration program.
* The recommended size of the ball for calibration is 40 millimeters. To improve accuracy, the ball should be attached 20 to 25 cm away from the gripper's TCP. 

.. Attention::
    Due to the robotic vibration, the ball's coordinates displayed in the camera will be distorted, reducing the accuracy of the final result significantly. It is strongly recommended that you connect the ball and the gripper with a solid object rather than a bendable object. 
    
* When attaching the ball onto the gripper, the stick cannot be aligned to the X,Y, and Z axis of the flange. Orientations should occur on 3 axis in 100mm,100mm, and 100 mm respectively. 
* The ball should be positioned within the camera's optimal working distance. When generating the ball's coordinating system, the initial pose should always be in the center of the camera's field of view. Then, either clockwise or counterclockwise, gradually adjust the ball's position around the first pose to create at least eight additional poses. The more poses you generate, the more precise the result.

.. image:: Sphere_Calibration/Sphere_Calibration_image/Picture1.png
    :align: center
     
|
.. Attention:: 
    To ensure the ball is centered in the camera, you can enable the crosshair function in the view section display setting dialogue, which will center the ball in the cross. 

.. image:: Sphere_Calibration/Sphere_Calibration_image/Picture2.png
    :align: center
     
|
* When generating the pose for the ball, it is necessary to have both X and Y orientations exerted on the flange. 
* Each pose generated should maintain the same altitude. Too much height deviation will have a significant impact on the final result's accuracy.
* In the software, you to adjust the value loaded inside the “Manage Variable” dialog based on the number of poses used in the calibration. 
  
.. image:: Sphere_Calibration/Sphere_Calibration_image/Picture3.png
    :align: center
    
|
Flowchart Setup
~~~~~~~~~~~~~~~~

The sphere calibration template comprises four different flowcharts: manual, calibration, SC_Eye_to_hand, and SC_Eye_in_hand. Each flowchart performs a distinct and crucial role inside the template.

.. image:: Sphere_Calibration/Sphere_Calibration_image/image2/Picture5.png
    :align: center
|

The manual flowchart is the first flowchart that must be executed. The flowchart's purpose is to manage and save the camera's data, which includes the image, point cloud, depth image, camera intrinsic, and pose. 

The vision software will decide which case to execute based on the command constants received from the robot. In general, you must initiate a handshake between the robot and the vision software first. Once the connection is established, the vision software uses the Assemble Bag node to trigger the camera to capture the image and save the data. From the Writer Node, the data will be assembled into a bag file.

You can specify the mode of generating the bag file within the writer node. By selecting the source type into "From file", a single bag file can be saved. 

.. image:: Sphere_Calibration/Sphere_Calibration_image/image2/Picture6.png
    :align: center
|

If there is a sequence of bag files, you can select “From Numbered” by defining the folder path and the file name along with its start and end index for each file. 

.. Attention:: 
    Normally we would set the end index into a large number to avoid editing the box in the future if you want to add more poses for the calibration. 

.. image:: Sphere_Calibration/Sphere_Calibration_image/image2/Picture7.png
    :align: center
|

Until all poses are executed, the flowchart will continuously run the second case for each pose. The flowchart will then transition to case three and terminate. All bag files will be saved in the user-defined folder path.

After running the manual flowchart, you must import the bag file folder that was previously saved into the Calibration flowchart in order to generate the yml file that will be used for the actual bin-picking applications. The yml file specifies the accuracy threshold for locating the object in the image. The lower the value, the more precise. 

.. image:: Sphere_Calibration/Sphere_Calibration_image/image2/Picture8.png
    :align: center
|

You can either manually filtering out the ball, or using the color filtering option to select the ball's cloud automatically. 

Manually filter the ball
~~~~~~~~~~~~~~~~~~~~~~~~~


    Inside the Reader node, uses the Folder path to load the bag file. 

.. image:: Sphere_Calibration/Sphere_Calibration_image/image2/Picture9.png
    :align: center
|

    After disassembling the bag file, using the cloud process node, adjust the Bounding Box to select the ball's point cloud in the scene. 

.. image:: Sphere_Calibration/Sphere_Calibration_image/image2/Picture10.png
    :align: center

.. image:: Sphere_Calibration/Sphere_Calibration_image/image2/Picture11.png
    :align: center
|

Auto selecting
~~~~~~~~~~~~~~~

    To speed up the process of finding the sphere cloud, you can also use the color filter function in the operation list to exclude the colors which do not belong to the targeting object or just include the color of the aiming object. 

.. image:: Sphere_Calibration/Sphere_Calibration_image/image2/Picture12.png
    :align: center
|

Inside the 3D Object Finder, you need to load the model's mesh into the node. Based on the requirement, you can configure feature detail and sample strength in the dialog as well. 

.. image:: Sphere_Calibration/Sphere_Calibration_image/image2/Picture13.png
    :align: center
|

After defining the model, you can use it to map the targeting object in the scene. You can define multiple models for associating objects of varying shapes. 

.. image:: Sphere_Calibration/Sphere_Calibration_image/image2/Picture14.png
    :align: center
|
You can use the Alignment node to map the model and point cloud even better. 

.. image:: Sphere_Calibration/Sphere_Calibration_image/image2/Picture15.png
    :align: center
|

In the first Sphere Calibration node, the final pairing data will be accumulated. To achieve a better result, you must adjust the Hand Eye Config to the desired mode and type in the value measured previously in the Sphere in Gripper Fields. 

.. Attention:: 
    To enable Eye-in-hand operation, change the Hand Eye Configuration to eye-in-hand.

The flowchart will iterate indefinitely until the number of poses is reached and the loop is broken. The final yml file will be generated in the template folder's "sphere calibrations" folder. 

.. image:: Sphere_Calibration/Sphere_Calibration_image/image2/Picture16.png
    :align: center
|

.. Attention:: 
    Each time you generate a new yml file, you should change the file name to avoid overwriting the previous one.


.. image:: Sphere_Calibration/Sphere_Calibration_image/image2/Picture17.png
    :align: center
|

Validation
~~~~~~~~~~~~~

The SC_Eye_to_hand and SC_Eye_in_hand flowcharts are similar to each other. Regarding to the application kind, you can select between the two flowcharts to visualize the final result based on application type. 

.. image:: Sphere_Calibration/Sphere_Calibration_image/image1/Picture18.png
    :align: center
.. image:: Sphere_Calibration/Sphere_Calibration_image/image1/Picture19.png
    :align: center
|

In both flowcharts, the yml file should be loaded into the Sphere Calibration Node. Using the first Reader and the second Reader node to load bag file and gripper mesh. You should be able to check how well the gripper mesh matches with the point cloud in the visualize node.  

For Eye-to-hand and Eye-in-hand applications, the output generated by the transformation is between sphere to cloud shown in Pic 20. You can validate the final result by viewing the mapping result between the sphere model and the sphere cloud point displayed in the scene cloud. However, in the Eye-to-hand flowchart, you can also see the pairing result by changing the output result from sphere to tool in the Transformation Tree output. 

.. image:: Sphere_Calibration/Sphere_Calibration_image/image1/Picture20.png
    :align: center
|

.. Attention:: 
    By recapturing the bag file for the gripper only and loading the same yml file generated previously, you can also check the pairing result of the tool's model mesh and scene cloud.  

For result validation, you need to recapture the gripper bag file by using different poses instead of using the original poses generated in the beginning. 

.. image:: Sphere_Calibration/Sphere_Calibration_image/image1/Picture21.png
    :align: center
|


Chessboard Calibration
----------------------

This algorithm attempts to calibrate the camera and the robot in order to determine the robot's relative position to the camera. After calibration, all coordinate systems will be accessible: Absolute (world), Camera, Gripper, and Robot Base. This enables the robot to position the gripper on a specific part of an object selected from the point cloud.


Preparation
~~~~~~~~~~~~~~

* To obtain the best results, it is necessary to thoroughly prepare the laboratory environment prior to running the chessboard calibration program.
* Attach the chessboard to the gripper's top.
  
.. Attention::  
    Due to the robotic vibration, the chessboard coordinates displayed in the camera will be distorted, reducing the accuracy of the final result significantly. It is strongly advised to reduce the robot's speed and set up the waiting time between each pose in order to capture a clear image.

* The chessboard should be positioned at the camera's optimal working distance. When generating the chessboard's coordinating system, the initial pose should always be in the center of the camera's field of view. Then, either clockwise or counterclockwise, gradually adjust the chessboard's position around the first pose to create at least eight additional poses. These poses should be angled approximately 30 degrees in relation to the Z direction of the world coordinates (as defined by the world points), and they should encompass the entire chessboard (even though the outer part of the chessboard may not contain any internal corners). Additionally, it is recommended to capture a small area around the chessboard to aid in the accuracy of later image corner detection. The more poses you generate, the more precise the result. 
* To ensure that the chessboard's initial pose is in the center of the camera, you can activate the crosshair function in the view section display setting dialogue, which will center the chessboard in the cross. The chessboard's maximum tilt angle is 40 degrees.
* The chessboard calibration can be done two ways: you can use the small circle or use the large circle to calibrate; however, each method has different approach to setup poses for generating the bag file as shown in the picture. 

Small circle

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/image/Picture23.png 
    :align: center
|
.. image:: Chessboard_Calibration/Chessboard_Calibration_image/image/Picture24.png
    :align: center
|
large circle 

.. Attention:: 
    If using the large circle orientation function inside the calibration, you can turn the chessboard or camera in 180 degrees from -90 to 90 depending on the application type as shown in the picture. For Eye-to-hand, the camera is placed in the center. The arrow shows the direction of the chessboard (Eye-to-hand) or the camera (Eye-in-hand) should be facing at each pose location. 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/image/Picture25.png
    :align: center
|

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/image/Picture26.png
    :align: center
|

* In comparison to the sphere calibration, the chessboard calibration requires more adjustments prior to running the flowchart. To begin, you must specify the number of poses to be used in the calibration. 
  
.. image:: Chessboard_Calibration/Chessboard_Calibration_image/image/Picture27.png
    :align: center
| 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/image/Picture28.png
    :align: center
| 

* Then you need to choose the calibration type depending on the application requirement. By default, the variable will be set to 1 as calibration.  
  
.. image:: Chessboard_Calibration/Chessboard_Calibration_image/image/Picture29.png
    :align: center
| 

* Lastly, you need to choose the approach for result validation, either using the pen on board, or gripper on TCP. 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/image/Picture30.png
    :align: center
| 

Flowchart Setup
~~~~~~~~~~~~~~~~

The first flowchart the needs to execute is the Manual flowchart. The purpose of the flowchart is to manage and save the data obtained from the camera including image, point cloud, depth image, camera intrinsic, and poses.

The vision software will decide to execute a specific case, based on the request command constants got from the robot. In general, you need to perform a handshake first between the robot and vision software.  Once the connection is made, the vision software will trigger the camera to capture the image and save the image data by using Assemble Bag node. The data will be assembled into a bag file from the Writer Node.

Inside the writer node, you can specify the mode for generating the bag file. A single bag file can be saved by selecting the source type into “From file”. 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/flowchart/Picture31.png
    :align: center
| 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/flowchart/Picture32.png
    :align: center
| 

If there is a sequence of bag files, you can select “From Numbered” by defining the folder path and the file name along with its start and end index for each file. 

.. Attention:: 
    Normally we would set the end index into a large number to avoid editing the box in the future if you want to add more poses for the calibration. 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/flowchart/Picture33.png
    :align: center
| 

The flowchart will be continuously running the second case for each pose until all the poses have been executed. Then, the flowchart will switch into case three and stop. All the bag files will be saved inside the folder path defined by you.

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/flowchart/Picture34.png
    :align: center
| 

Instead of manually setting up the pose and generating the bag file, you can autonomously set up everything by using the Auto flowchart.  

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/flowchart/Picture35.png
    :align: center
| 

You need to choose the moving path of the gripper, the distance from the camera, and the number of poses wanted to generate to the bag first.  In the circle field, you can adjust the tilting angle, height, and rotation var to adjust the pose of the chessboard.

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/flowchart/Picture36.png
    :align: center
| 

All bag files will be generated in the loop section and saved under the path defined in the writer node. 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/flowchart/Picture37.png
    :align: center
|    

After running the manual or auto flowchart, you need to load the bag file folder saved previously into the Calibration flowchart to generate yml file, which will be used for the real bin-picking applications. The yml file tells the threshold value of the accuracy of locating the object in the image. The smaller value, the better accuracy. 

Small circle method
~~~~~~~~~~~~~~~~~~~~~~

Inside the reader node, use the folder path to load the bag file generated from the previous step. 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/flowchart/Picture38.png
    :align: center
|    

Inside the node, you can select the source type they want for loading the bag file. If you chooses to load a file from Numbered, then it is necessary to define the parameter in the ADDITIONAL SETTINGS. 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/flowchart/Picture39.png
    :align: center
|   

All bag files will be accumulated in the first Calibration node. You should set up the number of rows, cols, spacing of the chessboard correctly to avoid mismatching between the bag file and the real board. 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/flowchart/Picture40.png
    :align: center
|   

The second Calibration node will generate the final yml used for the future application. You need to name the file by typing in the File Name section. 

Big circle method
~~~~~~~~~~~~~~~~~~

You can also select the Use large circle orientation function. However, it is still necessary to make sure to config the number of rows, columns, spacing between each row and column correctly. 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/flowchart/Picture41.png
    :align: center
|   

Validation
~~~~~~~~~~~~~~~~~~~

If the application type is Eye-to-hand, you should switch the flowchart to the Eye_to_hand. 

The first step is to load the testing bag file into the Reader node, and use the Disassemble Bag node to extract the data from inside. In the Calibration node, you need to type in the yml file name stored in the Calibration folder under the Chessboard Calibration template folder created by you. 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/validation/Picture42.png
    :align: center

|   

After the second switch, you need to load the gripper model they want to use based on the validation approach they chose in the beginning. The left sub-child path is to visualize how well the gripper model matches the gripper’s cloud. The right sub-child path is to visualize how good the world is located in the cloud. (The pen should plug right in the center of the first circle of the chessboard.) (Small circle) or check if the pen is located in the center of the central circle of the board. (Large circle)

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/validation/Picture43.png
    :align: center

|   

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/validation/Picture44.png
    :align: center

|   

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/validation/Picture45.png
    :align: center

|   

.. Attention:: 
    The testing bag file cannot be the same as the one used for generating the yml file. You need to readjust the poses and check the result using multiple bag files with different orientations. 


If the application type is Eye-in-hand, you should switch the flowchart to the Eye_in_hand. 

To validate the small circle method result, inside the first and second Readers, you needs to load the gripper model ply file and the bag file respectively, then use the Calibration node to load the yml file generated from the previous step. 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/validation/Picture46.png
    :align: center

|   

Different from the Eye-to-hand flowchart, you can only validate the result by using world in cloud method. 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/validation/Picture47.png
    :align: center

|   

To validate the large circle method result, the pen(world) should be located in the center of the scene cloud. 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/validation/Picture48.png
    :align: center

| 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/validation/Picture49.png
    :align: center

| 

.. image:: Chessboard_Calibration/Chessboard_Calibration_image/validation/Picture50.png
    :align: center

| 

.. Attention:: 
    For either Eye-to-hand or Eye-in-hand application, the testing bag file cannot be the same as the one used for generating the yml file. You need to readjust the poses and check the result using multiple bag files with different orientations.

2D Calibration
---------------

Overview
~~~~~~~~~~

2D Picking is to recognize object position in a 2D image, then guide the robot to move to the corresponding target in the real world.   

Preparation
~~~~~~~~~~~~

Overall, we need to set up one detection pose (for chessboard image capture), multiple waypoints for calibration, and multiple elevation waypoints to avoid the tcp from scratching the chessboard during the movement. Usually, we use three waypoints to locate the plane and use another 12 waypoints to generate the bag file. The elevation waypoints are added in between each waypoint. To set up the plane, the first waypoint should always start from the upper right corner. Then define the X-axis by selecting the point at the top left corner. Lastly, define the positive Y direction by selecting the point at the bottom left corner. 

.. image::  2D_Cali/2D_Calibration_Image/Capture1.png
    :align: center

| 

* When defining the points for generating the bag file, you should start by selecting the first bottom right corner point as the first waypoint, then set points as a “Z” shape towards the upper left corner. 
* When defining the 12 waypoints for the bag file, the z value relative to the plane should be as close to 0 as possible. If 0 is not able to reach, then keep the z value relative to the plane always the same number for all the waypoints. Then make sure to keep RX and RY equals 0 for all waypoints.
* For the twelve waypoints set up, you can follow the steps as shown below:
  
  .. image::  2D_Cali/2D_Calibration_Image/Capture2.png
    :align: center

| 

  #. For Detection Pose, make sure the chessboard is unblocked by robot arms.
  #. When setting up waypoints, make sure to select reference coordinate to the plane which has been defined.
  #. Change the unit from radiance into the degree, click into the TCP details and rotate Rz to (TCP/6th axis) -75°. 
  #. Then change z, Rx, Ry values to 0°.
  #. Save this waypoint.
  #. Edit the corresponding elevation point to around -5mm (the TCP has an opposite z value compared to the plane).
  #. Repeat the steps from 2 to 6 for the rest of the waypoints; however, you need to change the Rz rotation in the step 3 as the following degree mentions down below:
  
.. Attention:: 
      After changing the reference coordinate from the base to the plane, the Z value will be opposite to you controlling board interface. If you click on the moving up the bottom, the gripper will go down and wise visa. Without noticing, you may miss clicking the Up button causing the Tcp to thrust into the board causing any damage; more importantly, you have to redo the entire process since the Tcp various. 


.. list-table:: Angle Rotation Table
   :widths: 25 25
   :header-rows: 1

   * - Number of waypoints
     - Rotation Angles (Degree)
   * - 1
     - -75
   * - 2
     - 30 
   * - 3
     - 30
   * - 4
     - -15
   * - 5
     - 60 
   * - 6
     - 30 
   * - 7
     - -75
   * - 8
     - 30
   * - 9
     - 60
   * - 10
     - 30
   * - 11
     - -75
   * - 12
     - -75

.. Attention:: 
    Make sure the Chessboard is not moved during the setup. Check if Waypoint 1 still points exactly on the bottom left corner; moreover, all waypoints have the same z value relative to the plane, the threshold should be around + /- 0.5mm. For Rx and Ry, -0.5°<rx, ry<0.5°.

* Lastly, set the number of poses in the Manage Variable Dialog to 12. 

.. image::  2D_Cali/2D_Calibration_Image/Picture3.png
    :align: center

| 

Flowchart Setup
~~~~~~~~~~~~~~~~~~

In side the Manual flowchart, you should select where you want to save the chessboard image data bag. Each pose will be assembled and saved as a bag file. All files will be written into a folder where you defined previously. 

.. image::  2D_Cali/2D_Calibration_Image/Picture4.png
    :align: center
| 

.. Attention:: 
    Calibration board is not moved until this step is finished.

After finishing generating the bag file, use the Calibration flowchart to create the yml file and find out the relation between the Plane and the World (the Chessboard). 

Using the Reader node to browse the folder path and extract the robot pose bag files.

.. image::  2D_Cali/2D_Calibration_Image/Picture5.png
    :align: center
| 

Use the first Hand-Eye Calibration 2D V02 to load the bag files disassembled from the Reader Node. Then, use the second Hand-Eye Calibration 2D V02 to configure the Calibration Setting by setting the number of rows, columns, and spacing correctly. 

.. image::  2D_Cali/2D_Calibration_Image/Picture6.png
    :align: center
| 

you can find your calibration file under the workspace folder → pin_calibrations, if the "pin_calibrations" folder does not show up in Picking workspace, copy the folder or create a new folder and rename to “pin_calibrations“.

Validation
~~~~~~~~~~~

The accuracy (RMSE) can be checked in the console. The small number we get, the better result we obtained. 

.. image::  2D_Cali/2D_Calibration_Image/Picture7.png
    :align: center
| 


